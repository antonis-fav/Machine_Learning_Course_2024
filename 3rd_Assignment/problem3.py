# -*- coding: utf-8 -*-
"""problem3-mine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w1-r-K6j9sspVp6YMehFE833kCuxFx3G

## Εργασία 3 ##

Καλωσήρθατε στην τρίτη σας εργασία. Η εργασία αυτή έχει σκοπό να σας βοηθήσει να εμπεδώσετε τα σύνολα μοντέλων.

Στην εργασία αυτή θα πρέπει να συμπληρώσετε κώδικα Python 3 στα σημεία που αναφέρουν # YOUR CODE HERE. Μην τροποποιείτε τον κώδικα που βρίσκεται εκτός αυτών των περιοχών.

Πρωτού παραδόσετε την εργασία σας σιγουρευτείτε ότι ο κώδικας σε όλα τα κελιά τρέχει σωστά. Για το σκοπό αυτό επιλέξτε από το μενού Χρόνος εκτέλεσης (runtime) -> Επανεκίνηση περιόδου λειτουργίας και εκτέλεση όλων.

Συμπληρώστε το όνομα (NAME) και το AEM σας παρακάτω:
"""

NAME = "Enter your Name here"
AEM = "Enter your AEM here"

"""**1** Διαβάστε το διαθέσιμο από την sklearn σύνολο δεδομένων breast cancer, χωρίστε το σε δεδομένα εκπαίδευσης (X_train, y_train) και ελέγχου (X_test, y_test) σε ποσοστό 70%/30% αντίστοιχα με τη συνάρτηση train_test_split (τιμή για random_state βάλτε 0). Το σύνολο αφορά τη διάγνωση καρκίνου του μαστού με βάση μεταβλητές που υπολογίζονται από μια ψηφιοποιημένη εικόνα δείγματος μάζας μαστού που λήφθηκε μέσω αναρρόφησης λεπτής βελόνας (FNA). (2 μονάδες)"""

# YOUR CODE HERE
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

dataset = load_breast_cancer()
print(dataset.feature_names)
print(len(dataset.feature_names))
print(dataset.target_names)

X = dataset.data
y = dataset.target


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

display(X_train)

"""Τεστ ορθής ανάγνωσης και διαχωρισμού του συνόλου δεδομένων"""
assert round(X_train[0][8], 5) == 0.1779
assert round(X_test[0][8], 5) == 0.2116

"""**2** Υλοποιήστε μια ντετερμινιστική εκδοχή της μεθόδου των τυχαίων υποχώρων, η οποία χτίζει τόσα μοντέλα όσες και οι μεταβλητές εισόδου, κάθε ένα από τα οποία αγνοεί και μία διαφορετική μεταβλητή εισόδου. Π.χ. το πρώτο μοντέλο αγνοεί την πρώτη, το δεύτερο αγνοεί τη δεύτερη κτλ. Χρησιμοποιήστε τη συνάρτηση clone από το sklearn.base για να δημιουργήστε αντίγραφο του βασικού μοντέλου σε κάθε επανάληψη. (4 μονάδες)"""

import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.base import clone

class RandomSubspaceDet:
    def __init__(self, estimator=DecisionTreeClassifier()):
        # YOUR CODE HERE
        self.estimator = estimator

        return(None)

    def fit(self, X_train, y_train):
        # YOUR CODE HERE

        num_rows, num_cols = X_train.shape
        print("numbers of columns of X_train is :", num_cols)
        self.temp_estimator = []

        for i in range(0, num_cols):
          temp_train = np.delete(X_train, i, axis=1)
          self.temp_estimator.append(clone(estimator=self.estimator))
          self.temp_estimator[i].fit(temp_train, y_train)

        return(None)

    def predict(self, X):
        # YOUR CODE HERE
        n_rows, n_cols = X.shape

        zeroes = [0] * n_rows
        ones = [0] * n_rows
        final_pred = [0] * n_rows

        for i in range(0, n_cols):
          temp_test = np.delete(X, i ,axis=1)
          y_pred = self.temp_estimator[i].predict(temp_test)

          for j in range(0, n_rows):
              if y_pred[j] == 0:
                zeroes[j] += 1
              else:
                ones[j] += 1

        for i in range(0, n_rows):

            if zeroes[i] >= ones[i]:
              final_pred[i] = 0
            else:
              final_pred[i] = 1

        print(final_pred)

        return(final_pred)

"""Τεστ ορθής υλοποίησης RandomSubspaceDet"""
from sklearn.metrics import accuracy_score

rs = RandomSubspaceDet(estimator=DecisionTreeClassifier(random_state=1))
rs.fit(X_train, y_train)
assert round(accuracy_score(rs.predict(X_test), y_test), 4) == 0.9006

"""**3** Υλοποιήστε τη μέθοδο AdaBoost όπως παρουσιάστηκε στο μάθημα. Χρησιμοποιήστε τη συνάρτηση clone από το sklearn.base για να δημιουργήστε αντίγραφο του βασικού μοντέλου σε κάθε επανάληψη. Χρησιμοποιήστε την παράμετρο sample_weight της fit του βασικού μοντέλου για να ορίσετε τα βάρη των παραδειγμάτων εκπαίδευσης. (4 μονάδες)"""

import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.base import clone

class AdaBoost:
    def __init__(self, n_estimators=20, estimator=DecisionTreeClassifier(max_depth=1)):
        # YOUR CODE HERE
        self.N = n_estimators
        self.estimator = estimator

        return(None)

    def fit(self, X_train, y_train):
        # YOUR CODE HERE

        rows_train, cols_train = X_train.shape
        self.base_models = []
        self.e_t = [0] * self.N
        weights = [1 / rows_train] * rows_train  # Initial weight = 1 / (number of samples)
        for t in range(0, self.N):
          correct_count = 0
          wrong_count = 0
          s = 0
          correct_freq = 0
          wrong_freq = 0
          self.base_models.append(clone(self.estimator))
          self.base_models[t].fit(X_train, y_train, sample_weight=weights)
          y_pred = self.base_models[t].predict(X_train)

          for i in range(0, rows_train):
              if y_pred[i] != y_train[i]:
                  self.e_t[t] += weights[i]
                  # print("error is :", e_t[t])

          if (self.e_t[t] >= 0.5) | (self.e_t[t] == 0):
            break

          for i in range(0, rows_train):
            if y_pred[i] == y_train[i]:
                weights[i] = weights[i] * (self.e_t[t] / (1 - self.e_t[t]))
                correct_count += weights[i]
                correct_freq +=1
            else:
                wrong_count += weights[i]
                wrong_freq += 1

          norm_var = correct_count + wrong_count
          for i in range(0, rows_train):
            weights[i] = weights[i] / norm_var

        return(None)

    def predict(self, X):

      # YOUR CODE HERE
      rows_test, cols_test = X.shape
      ones = [0] * rows_test
      zeroes = [0] * rows_test
      final_pred = [0] * rows_test

      for t in range(0, self.N):
        y_pred_test = self.base_models[t].predict(X)

        for i in range(0, rows_test):
            if y_pred_test[i] == 0:
                zeroes[i] += np.log((1 - self.e_t[t]) / self.e_t[t])
            else:
                ones[i] += np.log((1 - self.e_t[t]) / self.e_t[t])

      for i in range(0, rows_test):
        if zeroes[i] >= ones[i]:
            final_pred[i] = 0
        else:
            final_pred[i] = 1

      return(final_pred)

"""Τεστ ορθής υλοποίησης AdaBoost"""
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score

ab = AdaBoost(n_estimators=20, estimator=DecisionTreeClassifier(max_depth=1, random_state=1))
ab.fit(X_train, y_train)
assert round(accuracy_score(ab.predict(X_test), y_test), 4) == 0.9591

# Ίδιο αποτέλεσμα και με τη κλάση της sklearn
ab = AdaBoostClassifier(n_estimators=20, algorithm="SAMME", estimator=DecisionTreeClassifier(max_depth=1, random_state=1))
ab.fit(X_train, y_train)
assert round(accuracy_score(ab.predict(X_test), y_test), 4) == 0.9591
